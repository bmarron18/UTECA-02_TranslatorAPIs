
#############
Info 
NLTK
##########
NLTK has been called a wonderful tool for teaching and working in computational linguistics using Python and an amazing library to play with natural language.



https://www.nltk.org/
https://www.nltk.org/book/
https://www.geeksforgeeks.org/nlp/nlp-bleu-score-for-evaluating-neural-machine-translation-python/

https://medium.com/data-science/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213
Before you get to that round, though, you’ll probably need to use at least one automatic evaluation metric. And I would urge you to use BLEU if and only if:
    You’re doing machine translation AND
    You’re evaluating across an entire corpus AND
    You know the limitations of the metric and you’re prepared to accept them.

https://thepythoncode.com/article/bleu-score-in-python



*	AI generated from Google query [generate blue score for a long text in python]------------
To calculate the BLEU score for a long text in Python, you can use the nltk.translate.bleu_score module, specifically the corpus_bleu function. This function is designed to handle multiple sentences (a corpus) and provides a more robust evaluation than sentence_bleu for longer texts. Both should be provided as (Python) lists of tokenized words [tokenize sentences ==> tokenize words] 
The BLEU score requires 
	reference texts (human-written translations or summaries)
	candidate text (the machine-generated text). 

--- Import necessary modules.
    from nltk.translate.bleu_score import corpus_bleu
    from nltk.tokenize import sent_tokenize, word_tokenize
	
--- Prepare your reference and candidate texts.
    reference_text_1 = "This is a long and complex reference text for evaluating the machine translation output. It contains multiple sentences and aims to provide a comprehensive example."
	
    reference_text_2 = "Another reference text that is also long and provides a different perspective on the same topic. This helps in capturing more nuances."
	
    candidate_text = "This long and complex text is a machine-generated translation for evaluation. It also contains several sentences and attempts to match the reference."

--- Tokenize the sentences in each text
    references = [
        [word_tokenize(sent) for sent in reference_text_1.split('. ') if sent],
        [word_tokenize(sent) for sent in reference_text_2.split('. ') if sent]
    ]
    candidate = [word_tokenize(sent) for sent in candidate_text.split('. ') if sent]


--- specify weights for n-grams (e.g., (0.25, 0.25, 0.25, 0.25) for 1-gram to 4-gram)
	weights=(0.25, 0.25, 0.25, 0.25)
	
--- calculate the BLEU score
    bleu_score = corpus_bleu(references, candidate, weights)

    print(bleu_score)




Explanation:

corpus_bleu(references, candidate, weights): 
	This function calculates the BLEU score across a corpus of sentences.
	
references: A list of reference documents. Each document is a list of tokenized sentences. In the example, we have two reference texts.

    references = [
        [tokenized_sentences_ref1], 
		[tokenized_sentences_ref2]
    ]

 
candidate: A list of tokenized sentences representing the machine-generated text.
		
weights: A tuple defining the weights for different n-grams (unigram, bigram, trigram, 4-gram). The default is (0.25, 0.25, 0.25, 0.25), meaning equal weight for each n-gram up to 4. You can adjust these weights based on your specific needs.



Tokenize---
Tokenize Sentences
	sent_tokenize() splits a string into a list of sentences, handling punctuation and abbreviations.

	from nltk.tokenize import sent_tokenize
	text = "NLTK is a great NLP toolkit. It makes processing text easy!"
	sentences = sent_tokenize(text)
	print(sentences)






##############
Install NLTK
in (spyder_env)
##################

*	Open (spyder_env)
	$ conda activate spyder_env

*	Install NLTK in (spyder_env)
	$ conda install nltk
		## Package Plan ##
			environment location: /home/bmarron/anaconda3/envs/spyder_env
		added / updated specs:
    		- nltk
The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    click-8.2.1                |     pyh707e725_0          86 KB  conda-forge
    colorama-0.4.6             |     pyhd8ed1ab_1          26 KB  conda-forge
    joblib-1.5.2               |     pyhd8ed1ab_0         219 KB  conda-forge
    nltk-3.9.1                 |     pyhd8ed1ab_1         1.0 MB  conda-forge
    regex-2025.9.1             |  py313h07c4f96_0         398 KB  conda-forge
    setuptools-80.9.0          |     pyhff2d567_0         731 KB  conda-forge
    tqdm-4.67.1                |     pyhd8ed1ab_1          87 KB  conda-forge
    ------------------------------------------------------------
                                           Total:         2.5 MB

The following NEW packages will be INSTALLED:

  click              conda-forge/noarch::click-8.2.1-pyh707e725_0 
  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_1 
  joblib             conda-forge/noarch::joblib-1.5.2-pyhd8ed1ab_0 
  nltk               conda-forge/noarch::nltk-3.9.1-pyhd8ed1ab_1 
  regex              conda-forge/linux-64::regex-2025.9.1-py313h07c4f96_0 
  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 
  tqdm               conda-forge/noarch::tqdm-4.67.1-pyhd8ed1ab_1 

	
	

*	Change Spyder to work in (spyder_env)
	(spyder_env) $ spyder
		In Spyder under "wrench" change Python interpreter [clic 'Apply' and 'OK']
			==> /home/bmarron/anaconda3/envs/spyder_env/bin/python


*	Restart Spyder [now running inside (spyder_env) Anaconda and w/o Spyder internal python]
	(spyder_env) $ spyder
		Python 3.13.7 | packaged by conda-forge | (main, Sep  3 2025, 14:30:35) [GCC 14.3.0]
		Type "copyright", "credits" or "license" for more information.

		IPython 8.37.0 -- An enhanced Interactive Python. Type '?' for help.

*	After NLTK is installed, download NLTK data (corpora, models, etc.) to use its full functionality. Th
	>>> import nltk
    >>> nltk.download()

This will open a graphical NLTK Downloader window where you can select specific packages or download the entire all collection
	[nltk data files here: /home/bmarron/nltk_data]

